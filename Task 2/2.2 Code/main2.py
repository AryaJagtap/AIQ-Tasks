# -*- coding: utf-8 -*-
"""scrappping .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RwS3AQum-IAz1td_IYKgXF-wdSzFUgTx
"""

# !pip install firecrawl-py

# pip install firecrawl-py

from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key="fc-4cb302d10cac47b5b22d863dddb35866")

from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key="fc-4cb302d10cac47b5b22d863dddb35866")

# Scrape a website:
doc = firecrawl.scrape("https://erail.in/train-running-status/12953", formats=["markdown", "html"])
print(doc)

from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key="fc-4cb302d10cac47b5b22d863dddb35866")

docs = firecrawl.crawl(url="https://www.railyatri.in/trains/seat-availability/12953", limit=10)
print(docs)

from firecrawl import Firecrawl
import pandas as pd
import time

# ----------------------------
# Initialize Firecrawl
# ----------------------------
firecrawl = Firecrawl(api_key="fc-4cb302d10cac47b5b22d863dddb35866")

# ----------------------------
# List of train URLs
# ----------------------------
train_urls = [
    "https://erail.in/train-running-status/12953",
    "https://erail.in/train-running-status/12001",
    # Add more train URLs here
]

all_data = []

# ----------------------------
# Loop through each train
# ----------------------------
for url in train_urls:
    # Introduce delay BEFORE making the API call to respect rate limits
    time.sleep(21) # Ensures at least 20 seconds between calls for a 3 req/min limit
    print(f"Scraping: {url}")
    try:
        # Start crawl
        job = firecrawl.crawl(url=url, limit=10)

        # Get results (list of Document objects) from job.data
        documents = job.data

        if documents:
            # Flatten results if nested or process each Document object
            for doc_obj in documents:
                # Each doc_obj is a Document object. You can access its attributes.
                # For now, let's append the whole Document object.
                # If this causes issues with DataFrame, we'll extract specific fields.
                all_data.append(doc_obj)

    except Exception as e:
        print(f"Error scraping {url}: {e}")

# ----------------------------
# Save all results to CSV
# ----------------------------
if all_data:
    df = pd.DataFrame(all_data)
    df.to_csv("all_trains_firecrawl.csv", index=False)
    print("✅ All train data saved to all_trains_firecrawl.csv")
else:
    print("No data scraped.")

# !pip install firecrawl-py pandas beautifulsoup4 markdown

from firecrawl import Firecrawl
import pandas as pd
from bs4 import BeautifulSoup
import markdown

firecrawl = Firecrawl(api_key="fc-4cb302d10cac47b5b22d863dddb35866")

# 1. List of train numbers you want to scrape
train_numbers = ["12953", "12954", "12009"]

all_rows = []

def extract_table_from_markdown(md_text):
    """Converts markdown -> HTML -> tables -> DataFrame"""
    try:
        html = markdown.markdown(md_text, extensions=['tables'])
        soup = BeautifulSoup(html, "html.parser")

        table = soup.find("table")
        if table is None:
            return None

        rows = []
        headers = [th.get_text(strip=True) for th in table.find_all("th")]

        for tr in table.find_all("tr")[1:]:
            cells = [td.get_text(strip=True) for td in tr.find_all("td")]
            if cells:
                rows.append(cells)

        return pd.DataFrame(rows, columns=headers)

    except Exception as e:
        print("Failed to parse markdown:", e)
        return None


# 2. Loop through train pages
for t in train_numbers:
    url = f"https://erail.in/train-running-status/{t}"
    print("Scraping train:", t)

    docs = firecrawl.scrape(url)

    md = docs.markdown
    if not md:
        print("No markdown found for", t)
        continue

    df = extract_table_from_markdown(md)
    if df is not None:
        df["train_number"] = t
        all_rows.append(df)
    else:
        print("No table extracted for", t)

# 3. Combine all
if all_rows:
    final_df = pd.concat(all_rows, ignore_index=True)
    final_df.to_csv("erail_trains.csv", index=False)
    print("\nSaved → erail_trains.csv")
else:
    print("No data extracted.")

# pip install firecrawl-py groq pandas

from firecrawl import Firecrawl
from groq import Groq
import pandas as pd
import json
import re # Import the regular expression module

firecrawl = Firecrawl(api_key="fc-4cb302d10cac47b5b22d863dddb35866")
groq_client = Groq(api_key="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")

def extract_structured(text):
    prompt = f"""
    Extract ALL meaningful railway data from below text.
    Return JSON with fields:
    {{
        "train_name": "",
        "train_number": "",
        "route": [
            {{"station": "", "arrival": "", "departure": ""}}
        ]
    }}

    TEXT:
    {text}

    Return ONLY JSON.
    """

    res = groq_client.chat.completions.create(
        model="llama-3.3-70b-versatile", # Updated to an active model from the list
        messages=[{"role": "user", "content": prompt}]
    )

    model_output = res.choices[0].message.content

    try:
        # Attempt to parse directly
        return json.loads(model_output)
    except json.JSONDecodeError:
        # If direct parsing fails, try to extract a JSON block using regex
        json_match = re.search(r'```json\n(.*)\n```', model_output, re.DOTALL)
        if json_match:
            json_string = json_match.group(1)
            try:
                return json.loads(json_string)
            except json.JSONDecodeError as e:
                raise ValueError(f"Failed to parse extracted JSON: {e}\nOriginal output: {model_output}")
        else:
            # If no JSON block is found, try to find a standalone JSON object
            json_match_alt = re.search(r'(\{[^}]*\})', model_output, re.DOTALL)
            if json_match_alt:
                json_string = json_match_alt.group(1)
                try:
                    return json.loads(json_string)
                except json.JSONDecodeError as e:
                    raise ValueError(f"Failed to parse alternative extracted JSON: {e}\nOriginal output: {model_output}")
            else:
                raise ValueError(f"No valid JSON found in model output.\nOriginal output: {model_output}")

def scrape_train(url):
    print("Scraping:", url)

    doc = firecrawl.scrape(url=url)
    text = doc.markdown or ""

    if not text:
        raise ValueError("Empty page content or markdown not available.")

    data = extract_structured(text)
    return data

def scrape_multiple(train_urls):
    all_data = []

    for url in train_urls:
        try:
            data = scrape_train(url)
            all_data.append(data)
        except Exception as e:
            print("Error:", e)

    return all_data

train_urls = [
    "https://erail.in/train-running-status/12953",
    "https://erail.in/train-running-status/12954",
    "https://erail.in/train-running-status/12951",
]

data = scrape_multiple(train_urls)

df = pd.json_normalize(data, record_path='route',
                       meta=['train_name', 'train_number'])

df.to_csv("trains.csv", index=False)

print("Saved \u2192 trains.csv")

import pandas as pd
display(df.sample(5))

from groq import Groq

groq_client = Groq(api_key="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")

print("Fetching available Groq models...")

try:
    models = groq_client.models.list()
    active_model_ids = [model.id for model in models.data]

    if active_model_ids:
        print("Available Groq Models:")
        for model_id in active_model_ids:
            print(f"- {model_id}")
        print("\nPlease select one of these active models and let me know.")
    else:
        print("No active Groq models found. Please check your Groq API key and account status.")

except Exception as e:
    print(f"Error fetching models: {e}")
    print("Please ensure your Groq API key is correct and you have network connectivity.")

import requests

url = "https://irctc1.p.rapidapi.com/api/v1/getTrainClasses"

querystring = {"trainNo":"19038"}

headers = {
	"x-rapidapi-key": "1099ed5049msh4c480808874066cp17acb8jsn157a51230658",
	"x-rapidapi-host": "irctc1.p.rapidapi.com"
}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())

